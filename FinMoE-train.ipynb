{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinMoE Training Notebook\n",
    "This notebook contains the code used to train FinMoE.</br></br>\n",
    "Developed by Samuel Barnett</br>\n",
    "Supervised by Dr JingJing Deng</br>\n",
    "Submitted as part of the degree of MEng Computer Science to the Board of Examiners in the Department of Computer Sciences, Durham University"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from huggingface_hub import constants as hub_c\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "from evals import evaluate_FinMoE, load_eval_dataset\n",
    "from FinMoE import FinMoE, FinMoEConfig\n",
    "from utils import DatasetArgs, get_dataset_args, load_train_datasets\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "ckpt_base = Path(r\"D:/models\")  # Path to directory containing expert ckpts\n",
    "dataset_cache_path = Path(r\"D:/datasets/FinMoE-train\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "args = get_dataset_args(tokenizer, Path(hub_c.HF_HUB_CACHE))\n",
    "\n",
    "model_name = \"FinMoE-final-top3-fast\" # new FinMoE model will be saved to this checkpoint in `ckpt_base`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Dataset\n",
    "Two preprocessing functions are provided for the two loss functions supported: ForCausalLM, and ForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess_causal(args: DatasetArgs, dataset_id: str, example: dict):\n",
    "    \"\"\"Provided for training with ForCausalLM loss\"\"\"\n",
    "    # Create prompt and target text\n",
    "    prompt_args = [example[key] for key in args.prompt_args[dataset_id]]\n",
    "    prompt = args.prompt_templates[dataset_id].format(*prompt_args)\n",
    "\n",
    "    target = args.id2labels[dataset_id][example[\"label\"]]\n",
    "\n",
    "    return tokenizer(prompt + target, truncation=False)\n",
    "\n",
    "\n",
    "def train_preprocess_tokenclass(args: DatasetArgs, dataset_id: str, example: dict):\n",
    "    \"\"\"Preprocessing function: applies prompt template to training sample and tokenizes prompt & label\"\"\"\n",
    "    # Create prompt and target text\n",
    "    prompt_args = [example[key] for key in args.prompt_args[dataset_id]]\n",
    "    prompt = args.prompt_templates[dataset_id].format(*prompt_args)\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized = tokenizer(prompt, truncation=False)\n",
    "\n",
    "    # tokenize and index label\n",
    "    target = args.id2labels[dataset_id][example[\"label\"]]\n",
    "    token_target = tokenizer.encode(target, add_special_tokens=False)[0]\n",
    "    label = args.token_list.index(token_target)\n",
    "    tokenized[\"labels\"] = label\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows_list = [3876, 3876, 3876] # ensure each dataset is equally represented in training dataset\n",
    "train_dataset = load_train_datasets(args, train_preprocess_tokenclass, nrows_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = dataset_cache_path.with_stem(\"finmoe-tokenclass_balanced-len256\")\n",
    "train_dataset.save_to_disk(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = dataset_cache_path.with_stem(\"finmoe-tokenclass_balanced-len256\")\n",
    "train_dataset = Dataset.load_from_disk(load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFY THESE CKPT NAMES\n",
    "expert_ckpt_names = {\"FPB\": \"checkpoint-best\",\n",
    "                     \"Headline\": \"checkpoint-best\",\n",
    "                     \"Topics\": \"checkpoint-best\"}\n",
    "\n",
    "## note: str() wraps path as Path objects are not json serializable\n",
    "# modify expert path names if necessary\n",
    "expert_ckpts = [str(ckpt_base / f\"expert-Llama-3_2-1B-{expert_name}\" / expert_ckpt_names[expert_name])\n",
    "                for expert_name in args.expert_order]\n",
    "\n",
    "loss_type = \"ForTokenClassification\" # \"ForCausalLM\"\n",
    "\n",
    "finMoE_config = FinMoEConfig(\n",
    "    loss_type=loss_type,\n",
    "    num_labels=len(args.token_list),\n",
    "\n",
    "    topk=\"top3\",\n",
    "    g_net_id=\"FastGating\",\n",
    "    expert_ckpts=expert_ckpts,\n",
    "    token_list=args.token_list,\n",
    ")\n",
    "\n",
    "data_collator = None\n",
    "if loss_type == \"ForCausalLM\":\n",
    "    ## use data_collator when training with \"ForCausalLM\" loss\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finMoE_model = FinMoE(finMoE_config).to(device)\n",
    "print(\"Memory allocated:\", torch.cuda.memory_allocated())\n",
    "print(\"Trainable params:\")\n",
    "for name, params in finMoE_model.named_parameters():\n",
    "    if params.requires_grad:\n",
    "        print(name, params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(ckpt_base / model_name),\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=128,\n",
    "    logging_steps=32,\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finMoE_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a model on a task, modify the `dataset_id` variable with the dataset name you want to evaluate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"Topics\"\n",
    "testset = load_eval_dataset(tokenizer, dataset_id, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate FinMoE checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = ckpt_base / model_name / \"checkpoint-3590\"\n",
    "finMoE_model = FinMoE.load_pretrained(ckpt_path).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_FinMoE(finMoE_model, tokenizer,\n",
    "                          testset,\n",
    "                          args.token_opts[dataset_id])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
