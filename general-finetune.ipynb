{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset, interleave_datasets\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling, PreTrainedModel\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "from evals import evaluate\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "seed = 42\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adaptllm_path(base_path: Path) -> Path:\n",
    "    with open(base_path / \"refs\" / \"main\", \"r\") as f_in:\n",
    "        snapshot_ref = f_in.readline()\n",
    "    return base_path / \"snapshots\" / snapshot_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load_dataset causes an error, load directly from cached snapshot files\n",
    "hub_basepath = Path(r\"C:\\Users\\samba\\.cache\\huggingface\\hub\")\n",
    "\n",
    "paths = {\n",
    "    \"FPB\": get_adaptllm_path(hub_basepath / \"datasets--AdaptLLM--FPB\"),\n",
    "    \"Headline\": get_adaptllm_path(hub_basepath / \"datasets--AdaptLLM--Headline\"),\n",
    "    \"Topics\": hub_basepath / r\"datasets--Sujet--TopicClassification\"\n",
    "}\n",
    "\n",
    "names_mapping = {\n",
    "    \"FPB\": None,\n",
    "    \"Headline\": [\"idx\", \"text\", \"question\", \"label\", \"subidx\"],\n",
    "    \"Topics\": [\"label\", \"text\"]\n",
    "}\n",
    "\n",
    "columns = {\n",
    "    \"FPB\": [\"text\", \"label\"],\n",
    "    \"Headline\": [\"idx\", \"text\", \"question\", \"label\", \"subidx\"],\n",
    "    \"Topics\": [\"label\", \"text\"]\n",
    "}\n",
    "\n",
    "del_mapping = {\n",
    "    \"FPB\": \"\\t\",\n",
    "    \"Headline\": \"\\t\",\n",
    "    \"Topics\": None ## regular comma-delimiter'd csv\n",
    "}\n",
    "\n",
    "\n",
    "topics = ['Analyst Update', 'Fed | Central Banks', 'Company | Product News', 'Treasuries | Corporate Debt', 'Dividend', 'Earnings', 'Energy | Oil', 'Financials', 'Currencies', 'General News | Opinion', 'Gold | Metals | Materials', 'IPO', 'Legal | Regulation', 'M&A | Investments', 'Macro', 'Markets', 'Politics', 'Personnel Change', 'Stock Commentary', 'Stock Movement']\n",
    "topic_options = \"\\n\".join([f\"{i} - {t}\" for i, t in enumerate(topics)])\n",
    "prompt_templates = {\n",
    "    \"FPB\": \"{0}\\nQuestion: what is the sentiment?\\nOptions:\\n- Positive\\n- Negative\\n- Neutral\",\n",
    "    \"Headline\": \"Headline: \\\"{0}\\\" Now answer this question: {1}\",\n",
    "    \"Topics\": \"{0}\\nNow classify the topic\\nOptions 0-19:\\n\" + f\"{topic_options} \",\n",
    "}\n",
    "\n",
    "prompt_args = {\n",
    "    \"FPB\": [\"text\"],\n",
    "    \"Headline\": [\"text\", \"question\"],\n",
    "    \"Topics\": [\"text\"],\n",
    "}\n",
    "\n",
    "id2labels = {\n",
    "    \"FPB\": {\"neutral\": \" Neutral\", \"positive\": \" Positive\", \"negative\": \" Negative\"},\n",
    "    \"Headline\": {0: \" No\", 1: \" Yes\"},\n",
    "    \"Topics\": {i: str(i) for i in range(20)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(dataset_id: str, example: dict):\n",
    "    # Create prompt and target text\n",
    "    args = [example[key] for key in prompt_args[dataset_id]]\n",
    "    prompt = prompt_templates[dataset_id].format(*args)\n",
    "\n",
    "    target = id2labels[dataset_id][example[\"label\"]]\n",
    "    full_text = prompt + target\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized = tokenizer(full_text,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=MAX_LENGTH)\n",
    "    \n",
    "    # add padding tokens\n",
    "    prompt_tokenized = tokenizer(prompt,\n",
    "                              truncation=True,\n",
    "                              max_length=MAX_LENGTH)\n",
    "    prompt_length = len(prompt_tokenized[\"input_ids\"])\n",
    "\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels[:prompt_length] = [-100] * prompt_length\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c510b069a238479d866b63fcd7153c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5045f5f557df47348e66d72d12f4d944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb301bf72144f1dbc7fc3ba4ed66f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_list = []\n",
    "for dataset_id, dataset_path in paths.items():\n",
    "    train_subset = pd.read_csv(dataset_path / \"train.csv\",\n",
    "                                delimiter=del_mapping[dataset_id],\n",
    "                                names=names_mapping[dataset_id])\n",
    "\n",
    "    preprocess_func = partial(train_preprocess, dataset_id)\n",
    "    dataset_list.append(Dataset\n",
    "                        .from_pandas(train_subset)\n",
    "                        .map(preprocess_func, batched=False)\n",
    "                        .remove_columns(columns[dataset_id]))\n",
    "\n",
    "n_datasets = len(dataset_list)\n",
    "train_dataset = interleave_datasets(dataset_list, \n",
    "                                    probabilities=[1/n_datasets]*n_datasets,\n",
    "                                    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peft Model trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\")\n",
    "\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "out_dir = Path(rf\"D:/models/general-Llama-3_2-3B\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FinMoE trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FinMoE import FinMoE, FinMoEConfig\n",
    "\n",
    "ckpt_base = Path(r\"D:\\models\")\n",
    "expert_ids = {\"FPB\": \"checkpoint-best\",\n",
    "              \"Headline\": \"checkpoint-best\",\n",
    "              \"Topics\": \"checkpoint-best\"}\n",
    "expert_ckpts = [ckpt_base / f\"expert-Llama-3_2-1B-{expert_name}\" / ckpt_name\n",
    "                for expert_name, ckpt_name in expert_ids.items()]\n",
    "\n",
    "finMoE_config = FinMoEConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    loss_type=\"ForCausalLM\",\n",
    "\n",
    "    expert_ckpts=expert_ckpts,\n",
    "    n_gate_layers=8,\n",
    ")\n",
    "\n",
    "finMoE_model = FinMoE(finMoE_config).to(device)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "out_dir = Path(rf\"D:/models/FinMoE-v1\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=6,\n",
    "    per_device_train_batch_size=1,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=finMoE_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35608e9ae5ee475495333cdfd5b5a2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 128000]' is invalid for input of size 65538816",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\transformers\\trainer.py:2122\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2120\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2123\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\transformers\\trainer.py:2474\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2474\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2477\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2479\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2480\u001b[0m ):\n\u001b[0;32m   2481\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2482\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\transformers\\trainer.py:3572\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3569\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3571\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3572\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3574\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3578\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\transformers\\trainer.py:3625\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3623\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3624\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3625\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3626\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\FinMoE.py:128\u001b[0m, in \u001b[0;36mFinMoE.forward\u001b[1;34m(self, input_ids, attention_mask, labels, **loss_kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutput(\n\u001b[0;32m    131\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m    132\u001b[0m     logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[0;32m    133\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    134\u001b[0m     attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    135\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\samba\\OneDrive - Durham University\\L4\\Project\\code\\venv\\Lib\\site-packages\\transformers\\loss\\loss_utils.py:42\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[1;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Flatten the tokens\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m shift_logits \u001b[38;5;241m=\u001b[39m \u001b[43mshift_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 128000]' is invalid for input of size 65538816"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36004e96ccd3479dbbf67bb138ee065c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ckpt_path = Path(rf\"D:/models/general-Llama-3_2-3B\") / \"checkpoint-best\"\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\") #.to(device)\n",
    "expert_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\").eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For FPB and Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_a(example, max_length=512):\n",
    "    zeroshot = example['input'].rsplit(\"\\n\\n\", maxsplit=1)[-1]\n",
    "    return tokenizer(zeroshot,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"FPB\"\n",
    "testset_adaptllm = load_dataset(\"AdaptLLM/finance-tasks\", dataset_id, split=\"test\").map(eval_preprocess_a, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58.56: 100%|██████████| 970/970 [07:05<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5855670103092784}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_options = {\n",
    "    \"FPB\": [59794, 45003, 51957],    # \" Neutral\", \" Positive\", \" Negative\"\n",
    "    \"Headline\": [7566, 2360],        # \" Yes\", \" No\"\n",
    "}\n",
    "\n",
    "results = evaluate(expert_model, tokenizer,\n",
    "                   testset_adaptllm,\n",
    "                   guidance=True,\n",
    "                   tok_opts=tok_options[dataset_id])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_b(example, max_length=512):\n",
    "    zeroshot = prompt_templates[\"Topics\"].format(example[\"text\"])\n",
    "    return tokenizer(zeroshot,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "topic_options = [str(i) for i in range(len(topics))]\n",
    "def add_options(example):\n",
    "    example[\"options\"] = topic_options\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d16df105bd4ecfa20901bdd9e5e9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b71203348f1473081f3e6a1d275fbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_id = \"Topics\"\n",
    "\n",
    "dataset_path = paths[dataset_id]\n",
    "testset_df = pd.read_csv(dataset_path / \"test.csv\",\n",
    "                            delimiter=del_mapping[dataset_id],\n",
    "                            names=names_mapping[dataset_id])\n",
    "testset_topics = (Dataset\n",
    "           .from_pandas(testset_df)\n",
    "           .map(eval_preprocess_b, batched=False)\n",
    "           .map(add_options, batched=False)\n",
    "           .rename_column(\"label\", \"gold_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_opts_ids = tokenizer(topic_options)[\"input_ids\"]\n",
    "tok_opts = [arr[1] for arr in tok_opts_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82.59: 100%|██████████| 850/850 [02:47<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8258823529411765}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(expert_model, tokenizer,\n",
    "                   testset_topics,\n",
    "                   guidance=True,\n",
    "                   tok_opts=tok_opts)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
