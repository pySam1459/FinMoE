{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, load_dataset, interleave_datasets\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling, PreTrainedModel\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "seed = 42\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load_dataset causes an error, load directly from cached snapshot files\n",
    "hub_basepath = Path(r\"C:\\Users\\samba\\.cache\\huggingface\\hub\")\n",
    "\n",
    "paths = {\n",
    "    \"FPB\": hub_basepath / r\"datasets--AdaptLLM--FPB\\snapshots\\7f203bd82f0b2b01ce391b9451c642dd732cf381\",\n",
    "    \"Headline\": hub_basepath / r\"datasets--AdaptLLM--Headline\\snapshots\\68cf1056f3ed51d39b945d004259473759555559\",\n",
    "    \"Topics\": hub_basepath / r\"datasets--Sujet--TopicClassification\"\n",
    "}\n",
    "\n",
    "names_mapping = {\n",
    "    \"FPB\": None,\n",
    "    \"Headline\": [\"idx\", \"text\", \"question\", \"label\", \"subidx\"],\n",
    "    \"Topics\": [\"label\", \"text\"]\n",
    "}\n",
    "\n",
    "columns = {\n",
    "    \"FPB\": [\"text\", \"label\"],\n",
    "    \"Headline\": [\"idx\", \"text\", \"question\", \"label\", \"subidx\"],\n",
    "    \"Topics\": [\"label\", \"text\"]\n",
    "}\n",
    "\n",
    "del_mapping = {\n",
    "    \"FPB\": \"\\t\",\n",
    "    \"Headline\": \"\\t\",\n",
    "    \"Topics\": None ## regular comma-delimiter'd csv\n",
    "}\n",
    "\n",
    "\n",
    "topics = ['Analyst Update', 'Fed | Central Banks', 'Company | Product News', 'Treasuries | Corporate Debt', 'Dividend', 'Earnings', 'Energy | Oil', 'Financials', 'Currencies', 'General News | Opinion', 'Gold | Metals | Materials', 'IPO', 'Legal | Regulation', 'M&A | Investments', 'Macro', 'Markets', 'Politics', 'Personnel Change', 'Stock Commentary', 'Stock Movement']\n",
    "topic_options = \"\\n\".join([f\"{i} - {t}\" for i, t in enumerate(topics)])\n",
    "prompt_templates = {\n",
    "    \"FPB\": \"{0}\\nQuestion: what is the sentiment?\\nOptions:\\n- Positive\\n- Negative\\n- Neutral\",\n",
    "    \"Headline\": \"Headline: \\\"{0}\\\" Now answer this question: {1}\",\n",
    "    \"Topics\": \"{0}\\nNow classify the topic\\nOptions 0-19:\\n\" + f\"{topic_options} \",\n",
    "}\n",
    "\n",
    "prompt_args = {\n",
    "    \"FPB\": [\"text\"],\n",
    "    \"Headline\": [\"text\", \"question\"],\n",
    "    \"Topics\": [\"text\"],\n",
    "}\n",
    "\n",
    "id2labels = {\n",
    "    \"FPB\": {\"neutral\": \" Neutral\", \"positive\": \" Positive\", \"negative\": \" Negative\"},\n",
    "    \"Headline\": {0: \" No\", 1: \" Yes\"},\n",
    "    \"Topics\": {i: str(i) for i in range(20)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(dataset_id: str, example: dict, max_length=512):\n",
    "    # Create prompt and target text\n",
    "    args = [example[key] for key in prompt_args[dataset_id]]\n",
    "    prompt = prompt_templates[dataset_id].format(*args)\n",
    "\n",
    "    target = id2labels[dataset_id][example[\"label\"]]\n",
    "    full_text = prompt + target\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized = tokenizer(full_text,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=max_length)\n",
    "    \n",
    "    # add padding tokens\n",
    "    prompt_tokenized = tokenizer(prompt,\n",
    "                              truncation=True,\n",
    "                              max_length=max_length)\n",
    "    prompt_length = len(prompt_tokenized[\"input_ids\"])\n",
    "\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels[:prompt_length] = [-100] * prompt_length\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ea70c989d44b7da7e047482bdda04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85201cb1b1b4b728eb0e25b8f0a1d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82161 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa0ea1dafafc46fb9ff6a785495f0882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_list = []\n",
    "for dataset_id, dataset_path in paths.items():\n",
    "    train_subset = pd.read_csv(dataset_path / \"train.csv\",\n",
    "                                delimiter=del_mapping[dataset_id],\n",
    "                                names=names_mapping[dataset_id])\n",
    "\n",
    "    preprocess_func = partial(train_preprocess, dataset_id)\n",
    "    dataset_list.append(Dataset\n",
    "                        .from_pandas(train_subset)\n",
    "                        .map(preprocess_func, batched=False)\n",
    "                        .remove_columns(columns[dataset_id]))\n",
    "\n",
    "n_datasets = len(dataset_list)\n",
    "train_dataset = interleave_datasets(dataset_list, \n",
    "                                    probabilities=[1/n_datasets]*n_datasets,\n",
    "                                    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 9\n",
    "batch_size = 2\n",
    "\n",
    "out_dir = Path(rf\"D:/models/general-Llama-3_2-LoRA\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=5000,\n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2763b35df4b64de692395079c525351e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1674, 'grad_norm': 10.780961036682129, 'learning_rate': 0.0001, 'epoch': 0.09}\n",
      "{'loss': 1.2148, 'grad_norm': 2.0696489810943604, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 1.1985, 'grad_norm': 5.538705825805664, 'learning_rate': 0.0003, 'epoch': 0.26}\n",
      "{'loss': 1.2013, 'grad_norm': 2.454968214035034, 'learning_rate': 0.0004, 'epoch': 0.35}\n",
      "{'loss': 1.1825, 'grad_norm': 2.95001482963562, 'learning_rate': 0.0005, 'epoch': 0.44}\n",
      "{'loss': 1.0664, 'grad_norm': 1.8794838190078735, 'learning_rate': 0.0006, 'epoch': 0.52}\n",
      "{'loss': 1.1549, 'grad_norm': 1.8944607973098755, 'learning_rate': 0.0007, 'epoch': 0.61}\n",
      "{'loss': 1.1203, 'grad_norm': 1.0156264305114746, 'learning_rate': 0.0008, 'epoch': 0.7}\n",
      "{'loss': 1.1481, 'grad_norm': 3.977154016494751, 'learning_rate': 0.0009000000000000001, 'epoch': 0.78}\n",
      "{'loss': 1.1451, 'grad_norm': 1.2982301712036133, 'learning_rate': 0.001, 'epoch': 0.87}\n",
      "{'loss': 1.2226, 'grad_norm': 2.102498769760132, 'learning_rate': 0.0009892945080826464, 'epoch': 0.96}\n",
      "{'loss': 1.1793, 'grad_norm': 1.9973130226135254, 'learning_rate': 0.0009785890161652928, 'epoch': 1.04}\n",
      "{'loss': 1.0973, 'grad_norm': 2.0318212509155273, 'learning_rate': 0.0009678835242479392, 'epoch': 1.13}\n",
      "{'loss': 1.1625, 'grad_norm': 1.1040457487106323, 'learning_rate': 0.0009571780323305856, 'epoch': 1.22}\n",
      "{'loss': 1.1511, 'grad_norm': 4.249826908111572, 'learning_rate': 0.000946472540413232, 'epoch': 1.31}\n",
      "{'loss': 1.1465, 'grad_norm': 6.564910411834717, 'learning_rate': 0.0009357670484958784, 'epoch': 1.39}\n",
      "{'loss': 1.1267, 'grad_norm': 1.756450891494751, 'learning_rate': 0.0009250615565785247, 'epoch': 1.48}\n",
      "{'loss': 1.1337, 'grad_norm': 2.3754162788391113, 'learning_rate': 0.0009143560646611711, 'epoch': 1.57}\n",
      "{'loss': 1.1227, 'grad_norm': 2.0358009338378906, 'learning_rate': 0.0009036505727438175, 'epoch': 1.65}\n",
      "{'loss': 1.149, 'grad_norm': 4.081614017486572, 'learning_rate': 0.0008929450808264639, 'epoch': 1.74}\n",
      "{'loss': 1.077, 'grad_norm': 1.609980821609497, 'learning_rate': 0.0008822395889091103, 'epoch': 1.83}\n",
      "{'loss': 1.137, 'grad_norm': 4.135726451873779, 'learning_rate': 0.0008715340969917568, 'epoch': 1.91}\n",
      "{'loss': 1.103, 'grad_norm': 1.2358373403549194, 'learning_rate': 0.0008608286050744032, 'epoch': 2.0}\n",
      "{'loss': 1.0592, 'grad_norm': 3.3341288566589355, 'learning_rate': 0.0008501231131570496, 'epoch': 2.09}\n",
      "{'loss': 1.0466, 'grad_norm': 3.1376736164093018, 'learning_rate': 0.000839417621239696, 'epoch': 2.18}\n",
      "{'loss': 1.0877, 'grad_norm': 1.2323875427246094, 'learning_rate': 0.0008287121293223424, 'epoch': 2.26}\n",
      "{'loss': 1.1055, 'grad_norm': 1.5791442394256592, 'learning_rate': 0.0008180066374049888, 'epoch': 2.35}\n",
      "{'loss': 1.073, 'grad_norm': 0.9397791624069214, 'learning_rate': 0.0008073011454876352, 'epoch': 2.44}\n",
      "{'loss': 1.1038, 'grad_norm': 2.68410062789917, 'learning_rate': 0.0007965956535702815, 'epoch': 2.52}\n",
      "{'loss': 1.0395, 'grad_norm': 1.1896685361862183, 'learning_rate': 0.0007858901616529279, 'epoch': 2.61}\n",
      "{'loss': 1.0263, 'grad_norm': 1.6797699928283691, 'learning_rate': 0.0007751846697355743, 'epoch': 2.7}\n",
      "{'loss': 1.0598, 'grad_norm': 3.5825366973876953, 'learning_rate': 0.0007644791778182207, 'epoch': 2.79}\n",
      "{'loss': 1.0596, 'grad_norm': 0.9084946513175964, 'learning_rate': 0.0007537736859008671, 'epoch': 2.87}\n",
      "{'loss': 1.0391, 'grad_norm': 1.7707911729812622, 'learning_rate': 0.0007430681939835135, 'epoch': 2.96}\n",
      "{'loss': 0.9985, 'grad_norm': 1.5975013971328735, 'learning_rate': 0.00073236270206616, 'epoch': 3.05}\n",
      "{'loss': 1.0124, 'grad_norm': 1.6680606603622437, 'learning_rate': 0.0007216572101488063, 'epoch': 3.13}\n",
      "{'loss': 1.001, 'grad_norm': 3.476025342941284, 'learning_rate': 0.0007109517182314528, 'epoch': 3.22}\n",
      "{'loss': 1.0018, 'grad_norm': 1.3808315992355347, 'learning_rate': 0.0007002462263140992, 'epoch': 3.31}\n",
      "{'loss': 0.9934, 'grad_norm': 1.2424660921096802, 'learning_rate': 0.0006895407343967456, 'epoch': 3.39}\n",
      "{'loss': 0.9685, 'grad_norm': 1.9348433017730713, 'learning_rate': 0.000678835242479392, 'epoch': 3.48}\n",
      "{'loss': 0.9711, 'grad_norm': 1.3736679553985596, 'learning_rate': 0.0006681297505620382, 'epoch': 3.57}\n",
      "{'loss': 0.9983, 'grad_norm': 1.393846869468689, 'learning_rate': 0.0006574242586446846, 'epoch': 3.66}\n",
      "{'loss': 0.9884, 'grad_norm': 2.4457037448883057, 'learning_rate': 0.000646718766727331, 'epoch': 3.74}\n",
      "{'loss': 0.9687, 'grad_norm': 1.2975993156433105, 'learning_rate': 0.0006360132748099775, 'epoch': 3.83}\n",
      "{'loss': 0.9789, 'grad_norm': 1.7400658130645752, 'learning_rate': 0.000625307782892624, 'epoch': 3.92}\n",
      "{'loss': 0.9377, 'grad_norm': 1.8911598920822144, 'learning_rate': 0.0006146022909752704, 'epoch': 4.0}\n",
      "{'loss': 0.9143, 'grad_norm': 3.777181625366211, 'learning_rate': 0.0006038967990579168, 'epoch': 4.09}\n",
      "{'loss': 0.9592, 'grad_norm': 3.459202289581299, 'learning_rate': 0.0005931913071405632, 'epoch': 4.18}\n",
      "{'loss': 0.926, 'grad_norm': 3.096309185028076, 'learning_rate': 0.0005824858152232096, 'epoch': 4.26}\n",
      "{'loss': 0.9132, 'grad_norm': 1.421823263168335, 'learning_rate': 0.000571780323305856, 'epoch': 4.35}\n",
      "{'loss': 0.9392, 'grad_norm': 2.0611894130706787, 'learning_rate': 0.0005610748313885024, 'epoch': 4.44}\n",
      "{'loss': 0.9538, 'grad_norm': 2.3116114139556885, 'learning_rate': 0.0005503693394711488, 'epoch': 4.53}\n",
      "{'loss': 0.9423, 'grad_norm': 1.536930799484253, 'learning_rate': 0.0005396638475537952, 'epoch': 4.61}\n",
      "{'loss': 0.9078, 'grad_norm': 2.2186410427093506, 'learning_rate': 0.0005289583556364415, 'epoch': 4.7}\n",
      "{'loss': 0.9156, 'grad_norm': 2.245243787765503, 'learning_rate': 0.0005182528637190879, 'epoch': 4.79}\n",
      "{'loss': 0.92, 'grad_norm': 2.760134220123291, 'learning_rate': 0.0005075473718017343, 'epoch': 4.87}\n",
      "{'loss': 0.9063, 'grad_norm': 1.6812831163406372, 'learning_rate': 0.0004968418798843807, 'epoch': 4.96}\n",
      "{'loss': 0.884, 'grad_norm': 3.1268789768218994, 'learning_rate': 0.00048613638796702707, 'epoch': 5.05}\n",
      "{'loss': 0.8919, 'grad_norm': 0.8758299946784973, 'learning_rate': 0.00047543089604967347, 'epoch': 5.13}\n",
      "{'loss': 0.8799, 'grad_norm': 1.3814536333084106, 'learning_rate': 0.0004647254041323199, 'epoch': 5.22}\n",
      "{'loss': 0.8553, 'grad_norm': 2.634665012359619, 'learning_rate': 0.0004540199122149663, 'epoch': 5.31}\n",
      "{'loss': 0.872, 'grad_norm': 3.5034866333007812, 'learning_rate': 0.00044331442029761273, 'epoch': 5.4}\n",
      "{'loss': 0.8714, 'grad_norm': 2.288715124130249, 'learning_rate': 0.0004326089283802591, 'epoch': 5.48}\n",
      "{'loss': 0.8756, 'grad_norm': 2.467571496963501, 'learning_rate': 0.0004219034364629055, 'epoch': 5.57}\n",
      "{'loss': 0.8409, 'grad_norm': 1.747098445892334, 'learning_rate': 0.0004111979445455519, 'epoch': 5.66}\n",
      "{'loss': 0.8499, 'grad_norm': 3.5672242641448975, 'learning_rate': 0.0004004924526281983, 'epoch': 5.74}\n",
      "{'loss': 0.8704, 'grad_norm': 1.4989874362945557, 'learning_rate': 0.0003897869607108447, 'epoch': 5.83}\n",
      "{'loss': 0.8779, 'grad_norm': 2.000039577484131, 'learning_rate': 0.0003790814687934911, 'epoch': 5.92}\n",
      "{'loss': 0.8189, 'grad_norm': 1.8424839973449707, 'learning_rate': 0.00036837597687613743, 'epoch': 6.01}\n",
      "{'loss': 0.8002, 'grad_norm': 1.375592589378357, 'learning_rate': 0.00035767048495878383, 'epoch': 6.09}\n",
      "{'loss': 0.8295, 'grad_norm': 2.2079639434814453, 'learning_rate': 0.00034696499304143023, 'epoch': 6.18}\n",
      "{'loss': 0.8143, 'grad_norm': 1.7330774068832397, 'learning_rate': 0.00033625950112407664, 'epoch': 6.27}\n",
      "{'loss': 0.8015, 'grad_norm': 1.0523170232772827, 'learning_rate': 0.0003255540092067231, 'epoch': 6.35}\n",
      "{'loss': 0.8095, 'grad_norm': 3.4065134525299072, 'learning_rate': 0.0003148485172893695, 'epoch': 6.44}\n",
      "{'loss': 0.8285, 'grad_norm': 1.045845627784729, 'learning_rate': 0.00030414302537201584, 'epoch': 6.53}\n",
      "{'loss': 0.803, 'grad_norm': 1.808298110961914, 'learning_rate': 0.00029343753345466224, 'epoch': 6.61}\n",
      "{'loss': 0.8212, 'grad_norm': 5.663008689880371, 'learning_rate': 0.00028273204153730864, 'epoch': 6.7}\n",
      "{'loss': 0.8001, 'grad_norm': 2.901972532272339, 'learning_rate': 0.00027202654961995504, 'epoch': 6.79}\n",
      "{'loss': 0.789, 'grad_norm': 0.7563633322715759, 'learning_rate': 0.00026132105770260145, 'epoch': 6.88}\n",
      "{'loss': 0.7847, 'grad_norm': 1.3172539472579956, 'learning_rate': 0.00025061556578524785, 'epoch': 6.96}\n",
      "{'loss': 0.7891, 'grad_norm': 0.7827136516571045, 'learning_rate': 0.00023991007386789422, 'epoch': 7.05}\n",
      "{'loss': 0.7475, 'grad_norm': 4.18356990814209, 'learning_rate': 0.00022920458195054065, 'epoch': 7.14}\n",
      "{'loss': 0.7932, 'grad_norm': 3.7523388862609863, 'learning_rate': 0.00021849909003318702, 'epoch': 7.22}\n",
      "{'loss': 0.7439, 'grad_norm': 2.0418930053710938, 'learning_rate': 0.00020779359811583343, 'epoch': 7.31}\n",
      "{'loss': 0.7485, 'grad_norm': 0.3072567284107208, 'learning_rate': 0.00019708810619847983, 'epoch': 7.4}\n",
      "{'loss': 0.7744, 'grad_norm': 1.4184634685516357, 'learning_rate': 0.0001863826142811262, 'epoch': 7.48}\n",
      "{'loss': 0.7617, 'grad_norm': 1.9954100847244263, 'learning_rate': 0.00017567712236377263, 'epoch': 7.57}\n",
      "{'loss': 0.7374, 'grad_norm': 1.7146998643875122, 'learning_rate': 0.00016497163044641903, 'epoch': 7.66}\n",
      "{'loss': 0.7323, 'grad_norm': 2.538222074508667, 'learning_rate': 0.0001542661385290654, 'epoch': 7.75}\n",
      "{'loss': 0.7643, 'grad_norm': 1.4589333534240723, 'learning_rate': 0.0001435606466117118, 'epoch': 7.83}\n",
      "{'loss': 0.7421, 'grad_norm': 2.0998711585998535, 'learning_rate': 0.0001328551546943582, 'epoch': 7.92}\n",
      "{'loss': 0.7558, 'grad_norm': 1.7533409595489502, 'learning_rate': 0.0001221496627770046, 'epoch': 8.01}\n",
      "{'loss': 0.7095, 'grad_norm': 2.769049644470215, 'learning_rate': 0.000111444170859651, 'epoch': 8.09}\n",
      "{'loss': 0.6783, 'grad_norm': 1.0417630672454834, 'learning_rate': 0.00010073867894229741, 'epoch': 8.18}\n",
      "{'loss': 0.6659, 'grad_norm': 2.578200340270996, 'learning_rate': 9.00331870249438e-05, 'epoch': 8.27}\n",
      "{'loss': 0.6819, 'grad_norm': 1.4502464532852173, 'learning_rate': 7.932769510759019e-05, 'epoch': 8.36}\n",
      "{'loss': 0.7146, 'grad_norm': 1.3885581493377686, 'learning_rate': 6.86222031902366e-05, 'epoch': 8.44}\n",
      "{'loss': 0.7035, 'grad_norm': 1.3986741304397583, 'learning_rate': 5.791671127288299e-05, 'epoch': 8.53}\n",
      "{'loss': 0.6911, 'grad_norm': 1.8213680982589722, 'learning_rate': 4.721121935552939e-05, 'epoch': 8.62}\n",
      "{'loss': 0.7171, 'grad_norm': 5.273733139038086, 'learning_rate': 3.650572743817578e-05, 'epoch': 8.7}\n",
      "{'loss': 0.6852, 'grad_norm': 0.9693002104759216, 'learning_rate': 2.5800235520822182e-05, 'epoch': 8.79}\n",
      "{'loss': 0.6719, 'grad_norm': 3.766249179840088, 'learning_rate': 1.509474360346858e-05, 'epoch': 8.88}\n",
      "{'loss': 0.7134, 'grad_norm': 3.140923500061035, 'learning_rate': 4.389251686114977e-06, 'epoch': 8.96}\n",
      "{'train_runtime': 18099.2647, 'train_samples_per_second': 5.713, 'train_steps_per_second': 2.857, 'train_loss': 0.9419316286552392, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=51705, training_loss=0.9419316286552392, metrics={'train_runtime': 18099.2647, 'train_samples_per_second': 5.713, 'train_steps_per_second': 2.857, 'total_flos': 3.0938841744408576e+17, 'train_loss': 0.9419316286552392, 'epoch': 9.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: PreTrainedModel,\n",
    "             testset: Dataset,\n",
    "             guidance = True,\n",
    "             tok_opts: Optional[list[int]] = None) -> dict[str, float]:\n",
    "\n",
    "    if guidance and tok_opts is None:\n",
    "        raise ValueError(\"Guidance requires token options\")\n",
    "\n",
    "    correct = 0\n",
    "    prog_bar = tqdm(testset)\n",
    "    for i, example in enumerate(prog_bar):\n",
    "        input_ids = torch.tensor(example[\"input_ids\"], device=device)\n",
    "        attn_mask = torch.tensor(example[\"attention_mask\"])\n",
    "        gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "\n",
    "        out = model.forward(input_ids=input_ids, attention_mask=attn_mask.to(device))\n",
    "        logits = out.logits.cpu()\n",
    "        \n",
    "        gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "        if guidance:\n",
    "            subset = gen_logits[0, tok_opts]\n",
    "            local_argmax = torch.argmax(subset).item()\n",
    "            gen_tokens = tok_opts[local_argmax]\n",
    "        else:\n",
    "            gen_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "\n",
    "        gen_raw = tokenizer.decode(gen_tokens).strip(\" \")\n",
    "        if example[\"options\"][example[\"gold_index\"]] == gen_raw:\n",
    "            correct += 1\n",
    "\n",
    "        prog_bar.set_description(f\"{100 * correct / (i+1):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": correct / len(testset)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For FPB and Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_a(example, max_length=512):\n",
    "    zeroshot = example['input'].rsplit(\"\\n\\n\", maxsplit=1)[-1]\n",
    "    return tokenizer(zeroshot,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b2ec1bc8164db887249f01776ed359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20547 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_id = \"Headline\"\n",
    "testset_adaptllm = load_dataset(\"AdaptLLM/finance-tasks\", dataset_id, split=\"test\").map(eval_preprocess_a, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83.07: 100%|██████████| 20547/20547 [1:18:29<00:00,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8307295468924903}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_options = {\n",
    "    \"FPB\": [59794, 45003, 51957],    # \" Neutral\", \" Positive\", \" Negative\"\n",
    "    \"Headline\": [7566, 2360],        # \" Yes\", \" No\"\n",
    "}\n",
    "\n",
    "ckpt_path = Path(rf\"D:/models/general-Llama-3_2-LoRA\") / \"checkpoint-best\"\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\")\n",
    "expert_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\").eval().to(device)\n",
    "\n",
    "results = evaluate(expert_model,\n",
    "                   testset_adaptllm,\n",
    "                   guidance=True,\n",
    "                   tok_opts=tok_options[dataset_id])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_b(example, max_length=512):\n",
    "    zeroshot = prompt_templates[\"Topics\"].format(example[\"text\"])\n",
    "    return tokenizer(zeroshot,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "topic_options = [str(i) for i in range(len(topics))]\n",
    "def add_options(example):\n",
    "    example[\"options\"] = topic_options\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cca4393539446188ccb886a876675f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5f0ef75313418b88c7217cb6820514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_id = \"Topics\"\n",
    "\n",
    "testset_df = pd.read_csv(dataset_path / \"test.csv\",\n",
    "                            delimiter=del_mapping[dataset_id],\n",
    "                            names=names_mapping[dataset_id])\n",
    "testset_topics = (Dataset\n",
    "           .from_pandas(testset_df)\n",
    "           .map(eval_preprocess_b, batched=False)\n",
    "           .map(add_options, batched=False)\n",
    "           .rename_column(\"label\", \"gold_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_opts_ids = tokenizer(topic_options)[\"input_ids\"]\n",
    "tok_opts = [arr[1] for arr in tok_opts_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85.53: 100%|██████████| 850/850 [04:39<00:00,  3.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8552941176470589}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = Path(rf\"D:/models/general-Llama-3_2-LoRA\") / \"checkpoint-best\"\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\") #.to(device)\n",
    "expert_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\").eval().to(device)\n",
    "\n",
    "results = evaluate(expert_model,\n",
    "                   testset_topics,\n",
    "                   guidance=True,\n",
    "                   tok_opts=tok_opts)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
