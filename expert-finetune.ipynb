{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorForLanguageModeling, PreTrainedModel\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from pathlib import Path\n",
    "\n",
    "from evals import evaluate\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\")\n",
    "\n",
    "dataset_id = \"FPB\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load_dataset causes an error, load directly from cached snapshot files\n",
    "hub_basepath = Path(r\"C:\\Users\\samba\\.cache\\huggingface\\hub\")\n",
    "\n",
    "paths = {\n",
    "    \"FPB\": hub_basepath / r\"datasets--AdaptLLM--FPB\\snapshots\\7f203bd82f0b2b01ce391b9451c642dd732cf381\",\n",
    "    \"Headline\": hub_basepath / r\"datasets--AdaptLLM--Headline\\snapshots\\68cf1056f3ed51d39b945d004259473759555559\",\n",
    "    \"Topics\": hub_basepath / r\"datasets--Sujet--TopicClassification\"\n",
    "}\n",
    "\n",
    "names_mapping = {\n",
    "    \"FPB\": None,\n",
    "    \"Headline\": [\"idx\", \"text\", \"question\", \"label\", \"subidx\"],\n",
    "    \"Topics\": [\"label\", \"text\"]\n",
    "}\n",
    "\n",
    "columns = {\n",
    "    \"FPB\": [\"text\", \"label\"],\n",
    "    \"Headline\": [\"idx\", \"text\", \"question\", \"label\", \"subidx\"],\n",
    "    \"Topics\": [\"label\", \"text\"]\n",
    "}\n",
    "\n",
    "del_mapping = {\n",
    "    \"FPB\": \"\\t\",\n",
    "    \"Headline\": \"\\t\",\n",
    "    \"Topics\": None\n",
    "}\n",
    "\n",
    "dataset_cols = {\n",
    "    \"FPB\": [\"text\"],\n",
    "    \"Headline\": [\"text\", \"question\"],\n",
    "    \"Topics\": [\"text\"],\n",
    "}\n",
    "\n",
    "topics = ['Analyst Update', 'Fed | Central Banks', 'Company | Product News', 'Treasuries | Corporate Debt', 'Dividend', 'Earnings', 'Energy | Oil', 'Financials', 'Currencies', 'General News | Opinion', 'Gold | Metals | Materials', 'IPO', 'Legal | Regulation', 'M&A | Investments', 'Macro', 'Markets', 'Politics', 'Personnel Change', 'Stock Commentary', 'Stock Movement']\n",
    "topic_options = \"\\n\".join([f\"{i} - {t}\" for i, t in enumerate(topics)])\n",
    "prompt_templates = {\n",
    "    \"FPB\": \"{0}\\nQuestion: what is the sentiment?\\nOptions:\\n- Positive\\n- Negative\\n- Neutral\",\n",
    "    \"Headline\": \"Headline: \\\"{0}\\\" Now answer this question: {1}\",\n",
    "    \"Topics\": \"{0}\\nNow classify the topic\\nOptions 0-19:\\n\" + f\"{topic_options} \",\n",
    "}\n",
    "\n",
    "id2labels = {\n",
    "    \"FPB\": {\"neutral\": \" Neutral\", \"positive\": \" Positive\", \"negative\": \" Negative\"},\n",
    "    \"Headline\": {0: \" No\", 1: \" Yes\"},\n",
    "    \"Topics\": {i: str(i) for i in range(20)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(example: dict, max_length=512):\n",
    "    # Create prompt and target text\n",
    "    args = [example[key] for key in dataset_cols[dataset_id]]\n",
    "    prompt = prompt_templates[dataset_id].format(*args)\n",
    "\n",
    "    target = id2labels[dataset_id][example[\"label\"]]\n",
    "    full_text = prompt + target\n",
    "\n",
    "    # tokenize text\n",
    "    tokenized = tokenizer(full_text,\n",
    "                          truncation=True,\n",
    "                          padding=\"max_length\",\n",
    "                          max_length=max_length)\n",
    "    \n",
    "    # add padding tokens\n",
    "    prompt_tokenized = tokenizer(prompt,\n",
    "                              truncation=True,\n",
    "                              max_length=max_length)\n",
    "    prompt_length = len(prompt_tokenized[\"input_ids\"])\n",
    "\n",
    "    labels = tokenized[\"input_ids\"].copy()\n",
    "    labels[:prompt_length] = [-100] * prompt_length\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d025f597ae5441d9b8be305b1f2fbe44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3876 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc41dd098b840f3a5ce397f7c9a5411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/970 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_path = paths[dataset_id]\n",
    "train_dataset = pd.read_csv(dataset_path / \"train.csv\",\n",
    "                            delimiter=del_mapping[dataset_id],\n",
    "                            names=names_mapping[dataset_id])\n",
    "test_dataset  = pd.read_csv(dataset_path / \"test.csv\",\n",
    "                            delimiter=del_mapping[dataset_id],\n",
    "                            names=names_mapping[dataset_id])\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_dataset).map(train_preprocess, batched=False).remove_columns(columns[dataset_id])\n",
    "val_dataset   = Dataset.from_pandas(test_dataset).map(train_preprocess, batched=False).remove_columns(columns[dataset_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Continue Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_ckpt = Path(rf\"D:/models/expert-Llama-3_2-1B-{dataset_id}\") / \"checkpoint-best\"\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_ckpt\n",
    ").to(device).train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "lr = 1e-3\n",
    "num_epochs = 15\n",
    "batch_size = 2\n",
    "\n",
    "out_dir = Path(rf\"D:/models/expert-Llama-3_2-1B-{dataset_id}-v2\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(out_dir),\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=5000,\n",
    "    save_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    # do_eval=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d852599e7db45a089fac39e85b53b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29070 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6319, 'grad_norm': 6.024759292602539, 'learning_rate': 0.0001, 'epoch': 0.26}\n",
      "{'loss': 1.8756, 'grad_norm': 2.8309383392333984, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 1.8123, 'grad_norm': 4.807130813598633, 'learning_rate': 0.0003, 'epoch': 0.77}\n",
      "{'loss': 1.78, 'grad_norm': 2.224982500076294, 'learning_rate': 0.0004, 'epoch': 1.03}\n",
      "{'loss': 1.7515, 'grad_norm': 2.7508039474487305, 'learning_rate': 0.0005, 'epoch': 1.29}\n",
      "{'loss': 1.7874, 'grad_norm': 3.5491952896118164, 'learning_rate': 0.0006, 'epoch': 1.55}\n",
      "{'loss': 1.769, 'grad_norm': 2.632551908493042, 'learning_rate': 0.0007, 'epoch': 1.81}\n",
      "{'loss': 1.7626, 'grad_norm': 4.6233391761779785, 'learning_rate': 0.0008, 'epoch': 2.06}\n",
      "{'loss': 1.7299, 'grad_norm': 2.7689712047576904, 'learning_rate': 0.0009000000000000001, 'epoch': 2.32}\n",
      "{'loss': 1.7742, 'grad_norm': 2.868759870529175, 'learning_rate': 0.001, 'epoch': 2.58}\n",
      "{'loss': 1.7814, 'grad_norm': 2.8815135955810547, 'learning_rate': 0.000979227253842958, 'epoch': 2.84}\n",
      "{'loss': 1.766, 'grad_norm': 4.007232189178467, 'learning_rate': 0.0009584545076859162, 'epoch': 3.1}\n",
      "{'loss': 1.7618, 'grad_norm': 5.614623546600342, 'learning_rate': 0.0009376817615288741, 'epoch': 3.35}\n",
      "{'loss': 1.7498, 'grad_norm': 2.5113136768341064, 'learning_rate': 0.0009169090153718321, 'epoch': 3.61}\n",
      "{'loss': 1.7537, 'grad_norm': 3.3775076866149902, 'learning_rate': 0.0008961362692147902, 'epoch': 3.87}\n",
      "{'loss': 1.6675, 'grad_norm': 6.3080244064331055, 'learning_rate': 0.0008753635230577483, 'epoch': 4.13}\n",
      "{'loss': 1.6543, 'grad_norm': 3.5198519229888916, 'learning_rate': 0.0008545907769007063, 'epoch': 4.39}\n",
      "{'loss': 1.6815, 'grad_norm': 4.85788631439209, 'learning_rate': 0.0008338180307436644, 'epoch': 4.64}\n",
      "{'loss': 1.675, 'grad_norm': 2.967228412628174, 'learning_rate': 0.0008130452845866224, 'epoch': 4.9}\n",
      "{'loss': 1.5913, 'grad_norm': 3.476496458053589, 'learning_rate': 0.0007922725384295803, 'epoch': 5.16}\n",
      "{'loss': 1.5852, 'grad_norm': 2.396674394607544, 'learning_rate': 0.0007714997922725384, 'epoch': 5.42}\n",
      "{'loss': 1.6205, 'grad_norm': 3.873047113418579, 'learning_rate': 0.0007507270461154965, 'epoch': 5.68}\n",
      "{'loss': 1.5884, 'grad_norm': 3.251370906829834, 'learning_rate': 0.0007299542999584546, 'epoch': 5.93}\n",
      "{'loss': 1.5005, 'grad_norm': 3.6683809757232666, 'learning_rate': 0.0007091815538014126, 'epoch': 6.19}\n",
      "{'loss': 1.4982, 'grad_norm': 3.3838388919830322, 'learning_rate': 0.0006884088076443706, 'epoch': 6.45}\n",
      "{'loss': 1.5304, 'grad_norm': 3.9064042568206787, 'learning_rate': 0.0006676360614873287, 'epoch': 6.71}\n",
      "{'loss': 1.5497, 'grad_norm': 2.7665231227874756, 'learning_rate': 0.0006468633153302866, 'epoch': 6.97}\n",
      "{'loss': 1.4046, 'grad_norm': 3.2399179935455322, 'learning_rate': 0.0006260905691732447, 'epoch': 7.22}\n",
      "{'loss': 1.4193, 'grad_norm': 4.290454864501953, 'learning_rate': 0.0006053178230162028, 'epoch': 7.48}\n",
      "{'loss': 1.462, 'grad_norm': 4.869609355926514, 'learning_rate': 0.0005845450768591608, 'epoch': 7.74}\n",
      "{'loss': 1.4445, 'grad_norm': 3.603562355041504, 'learning_rate': 0.0005637723307021188, 'epoch': 8.0}\n",
      "{'loss': 1.3003, 'grad_norm': 3.2456002235412598, 'learning_rate': 0.0005429995845450769, 'epoch': 8.26}\n",
      "{'loss': 1.3267, 'grad_norm': 2.7120938301086426, 'learning_rate': 0.0005222268383880349, 'epoch': 8.51}\n",
      "{'loss': 1.3759, 'grad_norm': 4.048810958862305, 'learning_rate': 0.0005014540922309929, 'epoch': 8.77}\n",
      "{'loss': 1.346, 'grad_norm': 3.1642894744873047, 'learning_rate': 0.00048068134607395096, 'epoch': 9.03}\n",
      "{'loss': 1.2095, 'grad_norm': 3.219223976135254, 'learning_rate': 0.00045990859991690904, 'epoch': 9.29}\n",
      "{'loss': 1.2476, 'grad_norm': 2.8925490379333496, 'learning_rate': 0.0004391358537598671, 'epoch': 9.55}\n",
      "{'loss': 1.2662, 'grad_norm': 4.859504222869873, 'learning_rate': 0.0004183631076028251, 'epoch': 9.8}\n",
      "{'loss': 1.2216, 'grad_norm': 3.202885866165161, 'learning_rate': 0.00039759036144578315, 'epoch': 10.06}\n",
      "{'loss': 1.1238, 'grad_norm': 4.327479362487793, 'learning_rate': 0.0003768176152887412, 'epoch': 10.32}\n",
      "{'loss': 1.1304, 'grad_norm': 4.193027019500732, 'learning_rate': 0.0003560448691316992, 'epoch': 10.58}\n",
      "{'loss': 1.1561, 'grad_norm': 4.828344345092773, 'learning_rate': 0.00033527212297465727, 'epoch': 10.84}\n",
      "{'loss': 1.113, 'grad_norm': 3.9432120323181152, 'learning_rate': 0.0003144993768176153, 'epoch': 11.09}\n",
      "{'loss': 1.0287, 'grad_norm': 3.7702901363372803, 'learning_rate': 0.0002937266306605733, 'epoch': 11.35}\n",
      "{'loss': 1.0377, 'grad_norm': 3.681734800338745, 'learning_rate': 0.0002729538845035314, 'epoch': 11.61}\n",
      "{'loss': 1.0437, 'grad_norm': 2.98056960105896, 'learning_rate': 0.0002521811383464894, 'epoch': 11.87}\n",
      "{'loss': 0.9781, 'grad_norm': 7.977772235870361, 'learning_rate': 0.00023140839218944745, 'epoch': 12.13}\n",
      "{'loss': 0.9085, 'grad_norm': 4.039342403411865, 'learning_rate': 0.00021063564603240547, 'epoch': 12.38}\n",
      "{'loss': 0.937, 'grad_norm': 4.547462463378906, 'learning_rate': 0.00018986289987536355, 'epoch': 12.64}\n",
      "{'loss': 0.9419, 'grad_norm': 5.493256092071533, 'learning_rate': 0.00016909015371832157, 'epoch': 12.9}\n",
      "{'loss': 0.8524, 'grad_norm': 3.5303266048431396, 'learning_rate': 0.0001483174075612796, 'epoch': 13.16}\n",
      "{'loss': 0.8104, 'grad_norm': 3.9818429946899414, 'learning_rate': 0.00012754466140423764, 'epoch': 13.42}\n",
      "{'loss': 0.8217, 'grad_norm': 4.694619178771973, 'learning_rate': 0.00010677191524719568, 'epoch': 13.67}\n",
      "{'loss': 0.8252, 'grad_norm': 4.442827224731445, 'learning_rate': 8.599916909015372e-05, 'epoch': 13.93}\n",
      "{'loss': 0.7465, 'grad_norm': 2.482818365097046, 'learning_rate': 6.522642293311177e-05, 'epoch': 14.19}\n",
      "{'loss': 0.7154, 'grad_norm': 5.338114261627197, 'learning_rate': 4.44536767760698e-05, 'epoch': 14.45}\n",
      "{'loss': 0.7146, 'grad_norm': 4.049196720123291, 'learning_rate': 2.3680930619027837e-05, 'epoch': 14.71}\n",
      "{'loss': 0.7265, 'grad_norm': 3.125152826309204, 'learning_rate': 2.9081844619858746e-06, 'epoch': 14.96}\n",
      "{'train_runtime': 10077.6843, 'train_samples_per_second': 5.769, 'train_steps_per_second': 2.885, 'train_loss': 1.382214455330622, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=29070, training_loss=1.382214455330622, metrics={'train_runtime': 10077.6843, 'train_samples_per_second': 5.769, 'train_steps_per_second': 2.885, 'total_flos': 1.739619789963264e+17, 'train_loss': 1.382214455330622, 'epoch': 15.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For FPB and Headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_a(example, max_length=512):\n",
    "    zeroshot = example['input'].rsplit(\"\\n\\n\", maxsplit=1)[-1]\n",
    "    return tokenizer(zeroshot,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_adaptllm = load_dataset(\"AdaptLLM/finance-tasks\", dataset_id, split=\"test\").map(eval_preprocess_a, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58.45: 100%|██████████| 970/970 [01:48<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5845360824742268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tok_options = {\n",
    "    \"FPB\": [59794, 45003, 51957],    # \" Neutral\", \" Positive\", \" Negative\"\n",
    "    \"Headline\": [7566, 2360],        # \" Yes\", \" No\"\n",
    "}\n",
    "\n",
    "ckpt_path = Path(rf\"D:/models/expert-Llama-3_2-1B-{dataset_id}-v2\") / \"checkpoint-best\"\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\").eval()\n",
    "expert_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\").eval().to(device)\n",
    "\n",
    "results = evaluate(base_model, tokenizer,\n",
    "                   testset_adaptllm,\n",
    "                   guidance=True,\n",
    "                   tok_opts=tok_options[dataset_id])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_preprocess_b(example, max_length=512):\n",
    "    zeroshot = prompt_templates[\"Topics\"].format(example[\"text\"])\n",
    "    return tokenizer(zeroshot,\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=max_length,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "topic_options = [str(i) for i in range(len(topics))]\n",
    "def add_options(example):\n",
    "    example[\"options\"] = topic_options\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "113c0dc7417e45e0a205fbdfb2c64019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35909ad96b0047429a2f421b74927336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testset_df = pd.read_csv(dataset_path / \"test.csv\",\n",
    "                            delimiter=del_mapping[dataset_id],\n",
    "                            names=names_mapping[dataset_id])\n",
    "testset_topics = (Dataset\n",
    "           .from_pandas(testset_df)\n",
    "           .map(eval_preprocess_b, batched=False)\n",
    "           .map(add_options, batched=False)\n",
    "           .rename_column(\"label\", \"gold_index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_opts_ids = tokenizer(topic_options)[\"input_ids\"]\n",
    "tok_opts = [arr[1] for arr in tok_opts_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85.18: 100%|██████████| 850/850 [03:35<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.851764705882353}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = Path(rf\"D:/models/expert-Llama-3_2-1B-Topics\") / \"checkpoint-best\"\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\") #.to(device)\n",
    "expert_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\").eval().to(device)\n",
    "\n",
    "results = evaluate(expert_model, tokenizer,\n",
    "                   testset_topics,\n",
    "                   guidance=True,\n",
    "                   tok_opts=tok_opts)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
