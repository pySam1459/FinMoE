{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "This notebook loads finance eval datasets and runs a model on each entry to gather a quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import BatchEncoding, pipeline\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "from typing import cast\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'To be or not to be: The impact of the pandemic on the future of the workplace\\nThe pandemic has forced the world to rethink its business strategies and the way it operates.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"To be or not to be\", pad_token_id=pipe.tokenizer.eos_token_id, max_new_tokens=30, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Using eos token as padding, can batch test cases to improve efficiency of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pipe.tokenizer\n",
    "assert tokenizer is not None\n",
    "\n",
    "def tokenize(text: str | list[str]) -> BatchEncoding:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer(text, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "def decode(tokens: torch.Tensor) -> str:\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20547"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsets = [\"FiQA_SA\", \"Headline\", \"ConvFinQA\", \"FPB\", \"NER\"]\n",
    "\n",
    "eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", \"Headline\")\n",
    "testset = eval_dataset[\"test\"]\n",
    "num_samples = len(testset)\n",
    "\n",
    "def get_batch(testset, offset: int, n: int) -> dict[str, list[str]]:\n",
    "    test_batch = testset[offset:offset+n]\n",
    "    return test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer=Yes, text=Headline: \"Gold falls to Rs 30,800; silver down at\n",
      "answer=No, text=Headline: february gold rallies to intraday high o\n",
      "answer=No, text=Please answer a question about the following headl\n",
      "answer=Yes, text=Read this headline: \"gold closes lower as dollar r\n",
      "answer=No, text=gold adds $42, or 2.4%, to trade at $1,833.30/oz\n",
      "Q\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch(0, 5)\n",
    "text, options, ids_ = batch[\"input\"], batch[\"options\"], batch[\"gold_index\"]\n",
    "\n",
    "for sample, opts, class_ in zip(text, options, ids_):\n",
    "    print(f\"answer={opts[class_]}, text={sample[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Yes No Yes Yes No'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cast(LlamaForCausalLM, pipe.model)\n",
    "model.eval()\n",
    "\n",
    "batch = get_batch(0, 5)\n",
    "\n",
    "## tokenize batch\n",
    "tokenized_input = tokenize(batch[\"input\"])\n",
    "input_ids = tokenized_input[\"input_ids\"].to(pipe.device)\n",
    "attn_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "## indices of the generated tokens for test samples in batch\n",
    "gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "\n",
    "## forward pass of LLM\n",
    "logits = (model\n",
    "            .forward(input_ids=input_ids, attention_mask=attn_mask.to(pipe.device))\n",
    "            .logits\n",
    "            .cpu()\n",
    "         ) # (B,T,C)\n",
    "\n",
    "## get generated output for each test sample\n",
    "gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "gen_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "\n",
    "## decode generated tokens\n",
    "decode(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = 0\n",
    "for tok, opts, class_id in zip(gen_tokens, batch[\"options\"], batch[\"gold_index\"]):\n",
    "    pred_opt = tokenizer.decode(tok).strip(\" \").lower()\n",
    "    num_correct += (pred_opt == opts[class_id].lower())\n",
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_batch(model: LlamaForCausalLM, test_batch: dict) -> int:\n",
    "    \"\"\"Run eval on model for a given test batch, returning the number of correct answers\"\"\"\n",
    "\n",
    "    tokenized_input = tokenize(test_batch[\"input\"])\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(pipe.device)\n",
    "    attn_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "    gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "    logits = (model\n",
    "                .forward(input_ids=input_ids, attention_mask=attn_mask.to(pipe.device))\n",
    "                .logits\n",
    "                .cpu()\n",
    "            )\n",
    "\n",
    "    gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "    gen_tokens = torch.argmax(gen_logits, dim=-1).tolist()\n",
    "    \n",
    "    num_correct = 0\n",
    "    for tok, opts, class_id in zip(gen_tokens, test_batch[\"options\"], test_batch[\"gold_index\"]):\n",
    "        pred_opt = tokenizer.decode(tok).strip(\" \").lower()\n",
    "        num_correct += (pred_opt == opts[class_id].lower())\n",
    "    \n",
    "    return num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: LlamaForCausalLM, testset, n: int, batch_size = 4, desc: str = \"\") -> float:\n",
    "    model.eval()\n",
    "    n_iters = n // batch_size\n",
    "\n",
    "    num_correct = 0\n",
    "    for k in tqdm(range(n_iters), desc=desc):\n",
    "        batch = get_batch(testset, k*batch_size, batch_size)\n",
    "        num_correct += eval_on_batch(model, batch)\n",
    "    \n",
    "    if n % batch_size:\n",
    "        batch = get_batch(testset, n_iters*batch_size, n)\n",
    "        num_correct += eval_on_batch(model, batch)\n",
    "    \n",
    "    return num_correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5136/5136 [39:11<00:00,  2.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6832140945150144"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", \"Headline\")\n",
    "evaluate(pipe.model, eval_dataset[\"test\"], len(testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval all subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "scores_file = open(f\"scores-{now_time}.txt\")\n",
    "\n",
    "subsets = [\"FiQA_SA\", \"Headline\", \"ConvFinQA\", \"FPB\", \"NER\"]\n",
    "for subset in subsets:\n",
    "    eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", subset)\n",
    "    eval_score = evaluate(pipe.model, eval_dataset[\"test\"], len(testset), subset)\n",
    "\n",
    "    out_txt = f\"{subset} = {eval_score*100:.2f}\"\n",
    "\n",
    "    print(out_txt)\n",
    "    scores_file.write(out_txt+\"\\n\")\n",
    "\n",
    "scores_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
