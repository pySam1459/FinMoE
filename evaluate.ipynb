{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "This notebook loads finance eval datasets and runs a model on each entry to gather a quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from transformers import BatchEncoding, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "# model_id = \"AdaptLLM/finance-LLM-13B\"\n",
    "# model_id = \"QuantFactory/finance-Llama3-8B-GGUF\"\n",
    "# model_id = \"instruction-pretrain/finance-Llama3-8B\"\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)#, use_fast=False)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\").to(device)\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = Path(r\"D:/models/basic-Llama-3_2-LoRA-phrasebank\") / \"checkpoint-8490\"\n",
    "ckpt_path = Path(r\"D:/models/basic-Llama-3_2-LoRA\") / \"checkpoint-12660\"\n",
    "lora_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\")\n",
    "lora_model.eval()\n",
    "\n",
    "model = lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Using eos token as padding, can batch test cases to improve efficiency of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str | list[str]) -> BatchEncoding:\n",
    "    return tokenizer(text,\n",
    "                     truncation=True,\n",
    "                     padding=True,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "def decode(tokens: torch.Tensor) -> str:\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(testset, offset: int, n: int) -> dict[str, list[str]]:\n",
    "    test_batch = testset[offset:offset+n]\n",
    "    return test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'options', 'gold_index', 'class_id'],\n",
       "    num_rows: 20547\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsets = [\"FiQA_SA\", \"Headline\", \"ConvFinQA\", \"FPB\", \"NER\"]\n",
    "\n",
    "eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", \"Headline\")\n",
    "testset = eval_dataset[\"test\"]\n",
    "num_samples = len(testset)\n",
    "\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer=Yes, text=Headline: \"Gold falls to Rs 30,800; silver down at\n",
      "answer=No, text=Headline: february gold rallies to intraday high o\n",
      "answer=No, text=Please answer a question about the following headl\n",
      "answer=Yes, text=Read this headline: \"gold closes lower as dollar r\n",
      "answer=No, text=gold adds $42, or 2.4%, to trade at $1,833.30/oz\n",
      "Q\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch(testset, 0, 5)\n",
    "text, options, ids_ = batch[\"input\"], batch[\"options\"], batch[\"gold_index\"]\n",
    "\n",
    "for sample, opts, class_ in zip(text, options, ids_):\n",
    "    print(f\"answer={opts[class_]}, text={sample[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No No No No No'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = get_batch(testset, 0, 5)\n",
    "\n",
    "## tokenize batch\n",
    "tokenized_input = tokenize(batch[\"input\"])\n",
    "input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "attn_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "## indices of the generated tokens for test samples in batch\n",
    "gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "\n",
    "## forward pass of LLM\n",
    "logits = (model\n",
    "            .forward(input_ids=input_ids, attention_mask=attn_mask.to(model.device))\n",
    "            .logits\n",
    "            .cpu()\n",
    "         ) # (B,T,C)\n",
    "\n",
    "## get generated output for each test sample\n",
    "gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "gen_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "\n",
    "## decode generated tokens\n",
    "decode(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = 0\n",
    "for tok, opts, class_id in zip(gen_tokens, batch[\"options\"], batch[\"gold_index\"]):\n",
    "    pred_opt = tokenizer.decode(tok).strip(\" \").lower()\n",
    "    num_correct += (pred_opt == opts[class_id].lower())\n",
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_batch(model: LlamaForCausalLM, test_batch: dict, is_tokenized=False) -> int:\n",
    "    \"\"\"Run eval on model for a given test batch, returning the number of correct answers\"\"\"\n",
    "\n",
    "    if not is_tokenized:\n",
    "        tokenized_input = tokenize(test_batch[\"input\"])\n",
    "    else:\n",
    "        tokenized_input = test_batch\n",
    "\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "    attn_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "    gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "    logits = (model\n",
    "              .forward(input_ids=input_ids, attention_mask=attn_mask.to(model.device))\n",
    "              .logits\n",
    "              .cpu())\n",
    "\n",
    "    gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "    gen_tokens = torch.argmax(gen_logits, dim=-1).tolist()\n",
    "\n",
    "    num_correct = 0\n",
    "    if \"label\" in test_batch:\n",
    "        for toks, label in zip(gen_tokens, test_batch[\"label\"]):\n",
    "            pred_label = tokenizer.decode(toks).strip(\" \").lower()\n",
    "            num_correct += (pred_label == label.lower())\n",
    "\n",
    "    elif \"options\" in test_batch:\n",
    "        for toks, opts, class_id in zip(gen_tokens, test_batch[\"options\"], test_batch[\"gold_index\"]):\n",
    "            pred_opt = tokenizer.decode(toks).strip(\" \").lower()\n",
    "            num_correct += (pred_opt == opts[class_id].lower())\n",
    "    \n",
    "    return num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: LlamaForCausalLM, testset, batch_size = 4, desc: str = \"\", is_tokenized=False) -> float:\n",
    "    model.eval()\n",
    "    n = len(testset)\n",
    "    n_iters = n // batch_size\n",
    "\n",
    "    num_total = 0\n",
    "    num_correct = 0\n",
    "    prog_bar = tqdm(range(n_iters), desc=desc)\n",
    "    for k in prog_bar:\n",
    "        batch = get_batch(testset, k*batch_size, batch_size)\n",
    "        num_correct += eval_on_batch(model, batch, is_tokenized=is_tokenized)\n",
    "        num_total += len(batch)\n",
    "        prog_bar.set_description(f\"{desc} | {100*num_correct/num_total:.3f}\")\n",
    "    \n",
    "    if n % batch_size:\n",
    "        batch = get_batch(testset, n_iters*batch_size, n)\n",
    "        num_correct += eval_on_batch(model, batch, is_tokenized=is_tokenized)\n",
    "    \n",
    "    return num_correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "def preprocess(examples):\n",
    "    label = id2label[examples[\"label\"]]\n",
    "    text = f\"Review: {examples['sentence']}\\nSentiment: {label}\"\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        # truncation=True,\n",
    "        # max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/566 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtakala/financial_phrasebank\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentences_allagree\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m eval_dataset\u001b[38;5;241m.\u001b[39mmap(preprocess)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_tokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, testset, batch_size, desc, is_tokenized)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m prog_bar:\n\u001b[0;32m     10\u001b[0m     batch \u001b[38;5;241m=\u001b[39m get_batch(testset, k\u001b[38;5;241m*\u001b[39mbatch_size, batch_size)\n\u001b[1;32m---> 11\u001b[0m     num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_tokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     num_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m     13\u001b[0m     prog_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mnum_correct\u001b[38;5;241m/\u001b[39mnum_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36meval_on_batch\u001b[1;34m(model, test_batch, is_tokenized)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     tokenized_input \u001b[38;5;241m=\u001b[39m test_batch\n\u001b[1;32m----> 9\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_input\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     10\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m tokenized_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     12\u001b[0m gen_idx \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", \"FPB\")\n",
    "eval_dataset = load_dataset(\"takala/financial_phrasebank\", \"sentences_allagree\", split=\"train\")\n",
    "eval_dataset = eval_dataset.map(preprocess)\n",
    "evaluate(model, eval_dataset, is_tokenized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval all subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FiQA_SA | 35.776: 100%|██████████| 58/58 [00:14<00:00,  4.01it/s]\n",
      "Headline | 52.652:   7%|▋         | 362/5136 [01:19<17:43,  4.49it/s]"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "scores_file = open(f\"scores-{now_time}.txt\", \"w\")\n",
    "\n",
    "subsets = [\"FiQA_SA\", \"Headline\", \"FPB\"] #  \"ConvFinQA\", \n",
    "subsets =  [\"FPB\"]\n",
    "for subset in subsets:\n",
    "    eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", subset)\n",
    "    eval_score = evaluate(model, eval_dataset[\"test\"], desc=subset)\n",
    "\n",
    "    out_txt = f\"{subset} = {eval_score*100:.2f}\"\n",
    "    scores_file.write(out_txt+\"\\n\")\n",
    "\n",
    "scores_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
