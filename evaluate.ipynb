{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "This notebook loads finance eval datasets and runs a model on each entry to gather a quantitative evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from transformers import BatchEncoding, AutoTokenizer\n",
    "from transformers.models.llama.modeling_llama import LlamaForCausalLM\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=\"float16\").to(device)\n",
    "model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = Path(r\"D:/models/basic-Llama-3_2-LoRA\") / \"checkpoint-12660\"\n",
    "lora_model = PeftModel.from_pretrained(base_model, ckpt_path, torch_dtype=\"float16\")\n",
    "lora_model.eval()\n",
    "\n",
    "model = lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Using eos token as padding, can batch test cases to improve efficiency of tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str | list[str]) -> BatchEncoding:\n",
    "    return tokenizer(text,\n",
    "                     truncation=True,\n",
    "                     padding=True,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "def decode(tokens: torch.Tensor) -> str:\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'input', 'options', 'gold_index', 'class_id'],\n",
       "    num_rows: 20547\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsets = [\"FiQA_SA\", \"Headline\", \"ConvFinQA\", \"FPB\", \"NER\"]\n",
    "\n",
    "eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", \"Headline\")\n",
    "testset = eval_dataset[\"test\"]\n",
    "num_samples = len(testset)\n",
    "\n",
    "def get_batch(testset, offset: int, n: int) -> dict[str, list[str]]:\n",
    "    test_batch = testset[offset:offset+n]\n",
    "    return test_batch\n",
    "\n",
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer=Yes, text=Headline: \"Gold falls to Rs 30,800; silver down at\n",
      "answer=No, text=Headline: february gold rallies to intraday high o\n",
      "answer=No, text=Please answer a question about the following headl\n",
      "answer=Yes, text=Read this headline: \"gold closes lower as dollar r\n",
      "answer=No, text=gold adds $42, or 2.4%, to trade at $1,833.30/oz\n",
      "Q\n"
     ]
    }
   ],
   "source": [
    "batch = get_batch(testset, 0, 5)\n",
    "text, options, ids_ = batch[\"input\"], batch[\"options\"], batch[\"gold_index\"]\n",
    "\n",
    "for sample, opts, class_ in zip(text, options, ids_):\n",
    "    print(f\"answer={opts[class_]}, text={sample[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No No No No No'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = get_batch(testset, 0, 5)\n",
    "\n",
    "## tokenize batch\n",
    "tokenized_input = tokenize(batch[\"input\"])\n",
    "input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "attn_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "## indices of the generated tokens for test samples in batch\n",
    "gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "\n",
    "## forward pass of LLM\n",
    "logits = (model\n",
    "            .forward(input_ids=input_ids, attention_mask=attn_mask.to(model.device))\n",
    "            .logits\n",
    "            .cpu()\n",
    "         ) # (B,T,C)\n",
    "\n",
    "## get generated output for each test sample\n",
    "gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "gen_tokens = torch.argmax(gen_logits, dim=-1)\n",
    "\n",
    "## decode generated tokens\n",
    "decode(gen_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = 0\n",
    "for tok, opts, class_id in zip(gen_tokens, batch[\"options\"], batch[\"gold_index\"]):\n",
    "    pred_opt = tokenizer.decode(tok).strip(\" \").lower()\n",
    "    num_correct += (pred_opt == opts[class_id].lower())\n",
    "num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_batch(model: LlamaForCausalLM, test_batch: dict) -> int:\n",
    "    \"\"\"Run eval on model for a given test batch, returning the number of correct answers\"\"\"\n",
    "\n",
    "    tokenized_input = tokenize(test_batch[\"input\"])\n",
    "    input_ids = tokenized_input[\"input_ids\"].to(model.device)\n",
    "    attn_mask = tokenized_input[\"attention_mask\"]\n",
    "\n",
    "    gen_idx = attn_mask.sum(dim=1).long() - 1\n",
    "    logits = (model\n",
    "                .forward(input_ids=input_ids, attention_mask=attn_mask.to(model.device))\n",
    "                .logits\n",
    "                .cpu()\n",
    "            )\n",
    "\n",
    "    gen_logits = logits[torch.arange(logits.size(0)), gen_idx, :] # (B, C)\n",
    "    gen_tokens = torch.argmax(gen_logits, dim=-1).tolist()\n",
    "    \n",
    "    num_correct = 0\n",
    "    for tok, opts, class_id in zip(gen_tokens, test_batch[\"options\"], test_batch[\"gold_index\"]):\n",
    "        pred_opt = tokenizer.decode(tok).strip(\" \").lower()\n",
    "        num_correct += (pred_opt == opts[class_id].lower())\n",
    "    \n",
    "    return num_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: LlamaForCausalLM, testset, batch_size = 4, desc: str = \"\") -> float:\n",
    "    model.eval()\n",
    "    n = len(testset)\n",
    "    n_iters = n // batch_size\n",
    "\n",
    "    num_total = 0\n",
    "    num_correct = 0\n",
    "    prog_bar = tqdm(range(n_iters), desc=desc)\n",
    "    for k in prog_bar:\n",
    "        batch = get_batch(testset, k*batch_size, batch_size)\n",
    "        num_correct += eval_on_batch(model, batch)\n",
    "        num_total += len(batch)\n",
    "        prog_bar.set_description(f\"{desc} | {100*num_correct/num_total:.3f}\")\n",
    "    \n",
    "    if n % batch_size:\n",
    "        batch = get_batch(testset, n_iters*batch_size, n)\n",
    "        num_correct += eval_on_batch(model, batch)\n",
    "    \n",
    "    return num_correct / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5136 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaptLLM/finance-tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeadline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, testset, batch_size, desc)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m prog_bar:\n\u001b[0;32m      9\u001b[0m     batch \u001b[38;5;241m=\u001b[39m get_batch(testset, k\u001b[38;5;241m*\u001b[39mbatch_size, batch_size)\n\u001b[1;32m---> 10\u001b[0m     num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     prog_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mnum_correct\u001b[38;5;241m/\u001b[39m(k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m%\u001b[39m batch_size:\n",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m, in \u001b[0;36meval_on_batch\u001b[1;34m(model, test_batch)\u001b[0m\n\u001b[0;32m      6\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m tokenized_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m gen_idx \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      9\u001b[0m logits \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\n\u001b[1;32m---> 12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m         )\n\u001b[0;32m     15\u001b[0m gen_logits \u001b[38;5;241m=\u001b[39m logits[torch\u001b[38;5;241m.\u001b[39marange(logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)), gen_idx, :] \u001b[38;5;66;03m# (B, C)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(gen_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", \"Headline\")\n",
    "evaluate(model, eval_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval all subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FiQA_SA | 35.776: 100%|██████████| 58/58 [00:14<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FiQA_SA = 36.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Headline | 52.492: 100%|██████████| 5136/5136 [17:54<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline = 65.62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c14815d2371f45b984c0b10d669a0bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.json:   0%|          | 0.00/5.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "980603ec17db46c08d89301c42d77882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ConvFinQA:   0%|          | 0/372 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subset \u001b[38;5;129;01min\u001b[39;00m subsets:\n\u001b[0;32m      8\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdaptLLM/finance-tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m, subset)\n\u001b[1;32m----> 9\u001b[0m     eval_score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     out_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_score\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(out_txt)\n",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, testset, batch_size, desc)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m prog_bar:\n\u001b[0;32m     10\u001b[0m     batch \u001b[38;5;241m=\u001b[39m get_batch(testset, k\u001b[38;5;241m*\u001b[39mbatch_size, batch_size)\n\u001b[1;32m---> 11\u001b[0m     num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43meval_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     num_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m     13\u001b[0m     prog_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mnum_correct\u001b[38;5;241m/\u001b[39mnum_total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36meval_on_batch\u001b[1;34m(model, test_batch)\u001b[0m\n\u001b[0;32m     16\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(gen_logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     18\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok, opts, class_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(gen_tokens, \u001b[43mtest_batch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, test_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgold_index\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     20\u001b[0m     pred_opt \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(tok)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m     21\u001b[0m     num_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred_opt \u001b[38;5;241m==\u001b[39m opts[class_id]\u001b[38;5;241m.\u001b[39mlower())\n",
      "\u001b[1;31mKeyError\u001b[0m: 'options'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now_time = datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n",
    "scores_file = open(f\"scores-{now_time}.txt\", \"w\")\n",
    "\n",
    "subsets = [\"FiQA_SA\", \"Headline\", \"ConvFinQA\", \"FPB\", \"NER\"]\n",
    "for subset in subsets:\n",
    "    eval_dataset = load_dataset(\"AdaptLLM/finance-tasks\", subset)\n",
    "    eval_score = evaluate(model, eval_dataset[\"test\"], desc=subset)\n",
    "\n",
    "    out_txt = f\"{subset} = {eval_score*100:.2f}\"\n",
    "\n",
    "    print(out_txt)\n",
    "    scores_file.write(out_txt+\"\\n\")\n",
    "\n",
    "scores_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
